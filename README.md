# Data Engineering & Analytics Portfolio  

Hi there! Welcome to my still developing data engineering & analytics portfolio. In this repository you will find projects completed from various data engineering/analytics courses or self development projects, each of which covers essential skills and techniques.  

## [Flight Data Pipeline](https://github.com/geoffreycloud/flight_data_pipeline)
- Summary: Apache Airflow was used to Automate an ETL pipeline extracting data hourly from [FlightLabs API](https://www.goflightlabs.com/real-time) transforming it, and loading it into a postgreSQL database.
- Technology used: _Python, pandas, psycopg2, PostgreSQL, Apache Airflow, Docker Desktop, Tableau_
- Final Results:
    - [Raw flight table](https://github.com/geoffreycloud/flight_data_pipeline/blob/main/data/flights_data_raw.csv) preserving API structure.
    - [Clean flight table](https://github.com/geoffreycloud/flight_data_pipeline/blob/main/data/flights_data_clean.csv) with normalized column names and transformations.  
 
## [Charlotte Parks & Greenways](https://github.com/geoffreycloud/ITIS-6112-Term-Project-Group-2-main)
_Role: Backend Developer_
- Summary: Designed and implemented backend services for an interactive web application that visualizes publicly available parks and recreation data in Mecklenburg County, NC, enabling efficient filtering, keyword search, and address-based radius queries. Focused on building reliable APIs and geospatial data workflows to support dynamic map interactions.
- Technology used: JavaScript, Node.js, Express.js, PostgreSQL (PostGIS), ArcGIS REST Services
- Final results: Scalable backend and geospatial database powering a user friendly interactive map for navigating county parks, trails, and greenways.  

## [Country GDP ETL Pipeline](https://github.com/geoffreycloud/3_ETL_GDP_Data)
Brief overview: Built an ETL pipeline as part of a data engineering course to scrape country GDP data from Wikipedia, clean and transform the values from millions to billions (USD), and store the processed data for analysis and verification. The focus was placed on web scraping techniques, data transformation, and structured data loading.
Technology used: _Python, Beautiful Soup, requests, pandas, SQLite, SQL_
Final results: [Cleaned and transformed GDP dataset]()
